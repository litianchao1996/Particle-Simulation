{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "963690b2",
      "metadata": {
        "id": "963690b2"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_2_3/a3_skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dde28458",
      "metadata": {
        "id": "dde28458"
      },
      "source": [
        "# Downloading Data and Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0580a5",
      "metadata": {
        "id": "7d0580a5"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import requests\n",
        "import io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce00edc",
      "metadata": {
        "id": "8ce00edc"
      },
      "outputs": [],
      "source": [
        "def load_zip(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    zipf = ZipFile(io.BytesIO(response.content))\n",
        "    return {name: zipf.read(name) for name in zipf.namelist()}\n",
        "\n",
        "def load_pickle(zipfile, fn):\n",
        "    return pickle.load(io.BytesIO(zipfile[fn]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb77a4be",
      "metadata": {
        "id": "bb77a4be"
      },
      "outputs": [],
      "source": [
        "data = load_zip('https://surfdrive.surf.nl/files/index.php/s/cwqGaS22KXgnXtg/download')\n",
        "\n",
        "    \n",
        "\"\"\"\n",
        "simulation_{train, valid, test} is stored as a list of simulations. \n",
        "Each simulation is a numpy array of size (t, 2): For t timesteps an x and y coordinate of our particle.\n",
        "\"\"\"\n",
        "simulation_train = load_pickle(data, 'data/train/simulation.pickle')  # 3.1 + 3.2\n",
        "simulation_valid = load_pickle(data, 'data/valid/simulation.pickle')  # 3.1 + 3.2\n",
        "simulation_test = load_pickle(data, 'data/test/simulation.pickle')  # 3.1 + 3.2\n",
        "\n",
        "\"\"\"\n",
        "charges_{train, valid, test} is stored as a list of simulation-charges. \n",
        "These charges are stored as numpy arrays of size (3,): One value for each charge.\n",
        "\"\"\"\n",
        "charges_train = load_pickle(data, 'data/train/charges.pickle')  # 3.1\n",
        "charges_valid = load_pickle(data, 'data/valid/charges.pickle')  # 3.1\n",
        "charges_test = load_pickle(data, 'data/test/charges.pickle')  # 3.1\n",
        "\n",
        "\"\"\"\n",
        "simulation_continued_{train, valid, test} is stored as a list of simulations. \n",
        "Each simulation is a numpy array of size (t, 2): For t timesteps an x and y coordinate of our particle.\n",
        "\"\"\"\n",
        "simulation_continued_train = load_pickle(data, 'data/train/simulation_continued.pickle')  # 3.2\n",
        "simulation_continued_valid = load_pickle(data, 'data/valid/simulation_continued.pickle')  # 3.2\n",
        "simulation_continued_test = load_pickle(data, 'data/test/simulation_continued.pickle')  # 3.2\n",
        "\n",
        "\"\"\"\n",
        "Note that the indices are shared throughout the different lists, e.g., for the 4th training simulation:\n",
        "simulation_train[3] contains its initial simulation\n",
        "charges_train[3] contains the charges associated with the simulation\n",
        "simulation_continued_train[3] contains the continuation of the simulation \n",
        "                --> simulation_continued_train[3][0] is the state after simulation_train[3][-1]\n",
        "\"\"\"\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a3438a",
      "metadata": {
        "id": "10a3438a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "31bc640b-afd7-415d-8e58-e5c9c1fec46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overview of no. datapoints:\n",
            "\n",
            "Task 3.1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-d85ef16f98a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Task 3.1:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{len(simulation_train)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{len(charges_train)} train, {len(charges_valid)} validation, {len(charges_test)} test charge pairs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ],
      "source": [
        "print('Overview of no. datapoints:\\n')\n",
        "\n",
        "print('Task 3.1:')\n",
        "print(f'{len(simulation_train)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations')\n",
        "print(f'{len(charges_train)} train, {len(charges_valid)} validation, {len(charges_test)} test charge pairs')\n",
        "print()\n",
        "\n",
        "print('Task 3.2:')\n",
        "print('Since len(simulation_continued_train) < len(simulation_train), we can only use a subset of initial simulations')\n",
        "print('We cut simulation_train down to the first 150 samples in simulation_train_task32')\n",
        "simulation_train_task32 = simulation_train[:150]\n",
        "print(f'{len(simulation_train_task32)} train, {len(simulation_valid)} validation, {len(simulation_test)} test simulations')\n",
        "print(f'{len(simulation_continued_train)} train, {len(simulation_continued_valid)} validation, {len(simulation_continued_test)} test continuations')\n",
        "\n",
        "print(f\"\"\"\n",
        "For task 3.1, use:\n",
        "{chr(10).join([\"simulation_{} + charges_{}\".format(t, t) for t in [\"train\", \"valid\", \"test\"]])}\n",
        "\n",
        "For task 3.2, use:\n",
        "{chr(10).join([\"simulation_{} + simulation_continued_{}\".format(*((t[0], t[1]) if isinstance(t, tuple) else (t, t))) for t in [(\"train_task32\", \"train\"), \"valid\", \"test\"]])}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cfafdb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cfafdb3",
        "outputId": "044a9591-1fb8-4ad6-a11d-f2d5dcb5aba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print some shapes:\n",
            "\n",
            "simulation_train[0].shape: (103, 2) -> (t, 2), (x, y) at every t)\n",
            "charges_train[0].shape: (3,) -> charges for the simulation\n",
            "simulation_continued_train[0].shape: (54, 2) -> (t, 2), (x, y) at every t)\n",
            "----\n",
            "\n",
            "simulation_train[1].shape: (97, 2) -> (t, 2), (x, y) at every t)\n",
            "charges_train[1].shape: (3,) -> charges for the simulation\n",
            "simulation_continued_train[1].shape: (45, 2) -> (t, 2), (x, y) at every t)\n",
            "----\n",
            "\n",
            "simulation_train[2].shape: (99, 2) -> (t, 2), (x, y) at every t)\n",
            "charges_train[2].shape: (3,) -> charges for the simulation\n",
            "simulation_continued_train[2].shape: (47, 2) -> (t, 2), (x, y) at every t)\n",
            "----\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Print some shapes:\\n')\n",
        "for i in range(3):\n",
        "    print('simulation_train[{}].shape:'.format(i), simulation_train[i].shape, '-> (t, 2), (x, y) at every t)')\n",
        "    print('charges_train[{}].shape:'.format(i), charges_train[i].shape, '-> charges for the simulation')\n",
        "    print('simulation_continued_train[{}].shape:'.format(i), simulation_continued_train[i].shape, '-> (t, 2), (x, y) at every t)')\n",
        "    print('----\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9106543",
      "metadata": {
        "id": "f9106543"
      },
      "outputs": [],
      "source": [
        "def plot_example(x, x_gt=None, x_pred=None, fn=None):\n",
        "    charge_locations = np.array([[-1.53846154, -1.53846154],\n",
        "                                 [ 1.53846154, -1.53846154],\n",
        "                                 [ 0.        ,  1.53846154]])  # charge locations are fixed\n",
        "    fig = plt.figure()\n",
        "    axes = plt.gca()\n",
        "    axes.set_xlim([-5., 5.])\n",
        "    axes.set_ylim([-5., 5.])\n",
        "    cmap = matplotlib.cm.get_cmap('tab20')\n",
        "    plt.plot(x[:, 0], x[:, 1], color=cmap(0))\n",
        "    plt.plot(x[0, 0], x[0, 1], 'd', color=cmap(1))\n",
        "    fig.set_size_inches(5, 5)\n",
        "    for charge in charge_locations:\n",
        "        plt.plot(charge[0], charge[1], 'd', color='black')\n",
        "    if x_gt is not None:\n",
        "        plt.plot(x_gt[:, 0], x_gt[:, 1], color='red', linewidth=.5)\n",
        "    if x_pred is not None:\n",
        "        plt.plot(x_pred[:, 0], x_pred[:, 1], color='green', linestyle='--')\n",
        "    if fn is None:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28681a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "d28681a6",
        "outputId": "73991a33-83e5-4441-a102-640486bb30bf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEvCAYAAAA6m2ZKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfqklEQVR4nO3deXQV9f3/8efn3pvkZoMASQiBQNhUwo4BwaUWa62KIrXuSlW01H71fPXULlJr25/WpWq1P1t/bV1obd1b6wqK4ILFVpBNJOxL2EMCgkBIyHI/vz9ugiwhuSGTO5PM63FOzknuzJ1538ucF5+Z+cznY6y1iIi0dwG3CxARiQeFnYj4gsJORHxBYScivqCwExFfUNiJiC+E3NhpZmamzc/Pd2PXItKOLViwYIe1NquhZa6EXX5+PvPnz3dj1yLSjhljNhxrmU5jRcQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfMGxsDPGBI0xi4wxbzm1TRERpzjZsrsVWO7g9kREHONI2BljegDjgKec2J6IiNOcatn9DvgJEHFoeyIijmpx2BljLgBKrbULmlhvsjFmvjFmfllZWUt3KyLSLE607E4DxhtjioEXgbOMMc8euZK19glrbaG1tjArK8uB3YqIxK7FYWetnWKt7WGtzQeuAN631l7T4spERBykfnYi4gshJzdmrf0Q+NDJbYqIOEEtOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWduCoSsVRW17pdhvhAyO0CxL827Cznu1PnsXlXBeOH5nLvtweRkqhDUlqHWnbimrteL+KL8iouK+zB64u3MPlvC6ipjbhdlrRTCjtxxc59B5izuozrT83n/ouH8MDFQ5izZge/f3+N26VJO6Wwk9ZRU9Po4mXb9hCxMLpvFwAuG5nHhGG5/PHDtawr2xePCsVnWhx2xpg8Y8wHxphlxpgiY8ytThQmbdyoUTBnzjEXF+/cD0CfzLSDr905roCkhAD3TV/R6uWJ/zjRsqsBbrfWFgCjgZuNMQUObFfasptuggcfhB07Gly8rzLa8uuYnHDwtaz0JCaf0YdZy7ezdMuXcSlT/KPFYWet3WatXVj3+15gOdC9pduVNu6aa6BnT7j/frD2qMUVVdGwCyccfghee1o+HcIhHntvdVzKFP9w9JqdMSYfGA7MdXK70galpECfPjBoELz44lGLQ8HooVcbOTwIO4QTmDimF7OWb2fTF/vjUqr4g2NhZ4xJA14BbrPW7mlg+WRjzHxjzPyysjKndite9r3vwZo1sHw5bNt22KKkUPTQq6w5uqvJ1af0AuDZuRtav0bxDUfCzhiTQDTonrPW/quhday1T1hrC621hVlZWU7sVrwuPR1yc2HChOj1u0NOZ1OTop2H91ZWH/W23IxkzinI4eVPN1HVQBiKHA8n7sYa4GlgubX2kZaXJO3KjTfC889HA++55w6+3LVDGIDtew40+LbLR+axa381s1fpLECc4UTL7jRgInCWMWZx3c/5DmxX2oOkpGg3FGNg9WrYuhWAnLqwK/myssG3nd4/ky6piby6aHPcSpX2zYm7sXOstcZaO8RaO6zuZ7oTxUk7ceml8M9/wo9+dPB0tltGNOy27K5o8C0JwQAXDs1l1vJSvqw4+lRXpLn0BIW0PmPguuvgpZfgkkvgmWfokppIp5QEVm/fe8y3XTCkG1U1EZ3KiiMUdhIfI0ZAcTEMGACbN2OKizkxJ50VJccOu+E9O9EpJYEPVpTGr05ptxR2Ej8//CE8/DD8+MfwyCMMyEpl1fa9RCJHdzoGCAYMY0/M5sOVpUf1xxNpLoWdxE/nzjB0KHz8MUyezIR/v8L+qtpjtu6Kiop45c7L2b5hDYs27opzsdLeKOwkvi6/HN54A/r2pVfPLIZvWcF/1+08arXy8nLOP/98Nq1bTek//w9zlm9xoVhpTxR2El/GRO/K/uY3ZNx6M1cV/5fFnxcftdqkSZMoLS3FWktk/27+eM+P4l+rtCsKO4m/Hj2iNypmzWLFjbcy6pnfU1P91fh3U6dOZdq0aVRWRvvg2Zoq1iz4iKlTp7pVsbQDCjtxxxVXwIwZnNovkxcGjGXDnfccXDRlyhTKy8sPWz1SVckdU6bEu0ppRxR24p477+TMl/7Elp79mFOVCq+9BsD9999PamrqYauahCR++LNfuVCktBcKO3FPp06EJl3Pfetn8ZsOQ6ha8jksWcKkSZMYN24c4XD0KYvEpDDJfUcx5rxLXC5Y2jKFnbhr2DBOGHECo5Z/wtsXXAd//zusXs3UqVPJzs7GGENmVhZdzru1wRFSRGKlsBPX9f3fyZy+q5hXP1oRHdn48cdJ3bmT6dOnU1BQwPP/eI1AYpg9FY1P4iPSGIWduC4QMPCLu/iwpIrPtu2D3/wGHnqIgZmZLF26lJEjhgCwRy07aQGFnXjC5aN6kp4U4sl/r4sOC/XAA3D33bBtG8kJQUIBwx6NfiItoLATT0gPJ3DVKT2Z/vk27n97OU8u2A4PPQT33YfZtIlwQpDKao1aLMcv5HYBIvWuOy2fp+es58+z1wHRhy1ufPhh+OlP6cswqmpzXa5Q2jK17MQzunVM5rKReRgDQ/My+PW05Uz9dCs89BCXLphO53Wr3C5R2jCFnXjKLWP7kRAI0D87jfMG5XD3W8v467zNPDnuewya8w7Mm+d2idJG6TRWPCU3I5krR+Xx7NyNzLjta0Ss5VdvLgPg9e/cxDmzZkB5OYwd63Kl0taoZSeec/PYfoQChj+8v5o/XDWC8wfnADDt8xL42c9gyRJ4802Xq5S2RmEnnpPdIcz1p/Xm9c+2smr7Xh67YvjBZb9/bzXceiuUlMA//uFildLWKOzEk35wZl86hBN48J2VhILRa3gAv525ikdmrsLeeCNUVkYfLxOJgcJOPKljSgI3j+3L7FVl/GftDkLBAN84KZvLCnvw2HurefjdldhrroFwGJ56yu1ypQ1Q2IlnfXdMPrkdwzzw9gqstQQChgcuHsKVo3ry+Adro69fcgl06QJPPul2ueJxCjvxrHBCkNvPOZElm79kRcleDNHnaO+dMIiJo3vx54/Wcd/05dgJEyAzU4EnjVLYiadNGN6dAd06AFBTN51iIGC4+6KBXDumF0/+ez0PvLPiq8B7+mk3yxUPU9iJpwUDhjvOOwmA9w+ZLNsYw6/GD+Sa0T358+x1PDRjZTTw0tLghRfcKlc8TGEnnve1/pkHfz90mCdjDHePH8SVo/L4fx+u5f++tzo6VeP+/dHpGkUOobATzzPGHPz9ibpBAupFr+EN5pKTe/C7Wat5es56uOEGWL8ePvgg3qWKhynspE3o2TkFgKfmrGP7nsrDlkXv0g7mvEE53PPWMl7+dFO04/Hs2VBU5Ea54kEKO2kTaiOWUfmdqY1Yfjfr6NFPQsEAv7tiGGf0z+SOfy3hnaUlcNdd0T5427a5ULF4jcJO2oTaiCU/M4WrT+nFy/M3s65s31HrJIWC/HniyQzNy+DWFxexYPOXcO+98OtfRwcPEF9T2EmbUBOJEAoGuOWsfiSFAvx2ZsNj26Ukhnj62pHkZiRz4zPzWVceibbwfvlLsDbOVYuXKOykTaiutSQEDJlpSdxwem+mLdnG0i1fNrhu59RE/nr9SALGcO1f5rEzrRNcdhk89licqxYvUdhJm1BdGyEhGD1cv/e1PmSkJPDQjJXHXL9Xl1Sevm4kpXsO8IPnFlI1ohA6dYIZM+JVsniMwk7ahJpaS6gu7DqEE7jpzOggAQs2fHHM9wzLy+DBS4Ywb/0X/PKNIuzEidE7tFu2xKts8RCFnXietZbqSISE4Ff97b47pheZaYk8OnN1o++9aFh3/ufrfXlh3kb+/skG+PnPo9M01ta2dtniMQo78byaiMVaSAx+dbimJIa46cy+zFmzg7nrdjb6/h+dcyJnD8jm7jeXsXDHAfj+9+HRR1u7bPEYhZ14XlVNdL7YxNDhh+vVp/QiKz2JRxvod3eoQMDw20uH0S0jzC3PLWRX7xOgsFB3Z31GYSeeV13bcNglJwa56cy+fLLuCz4tPva1O4gOBvr4VSPYsa+KH768mMjXzoxOTCu+obATz6tv2SUEjz5crxrVky6pifzh/TVNbmdIjwzuumAAH6wsY+rH6x2vU7xNYSeed6Au7JJCRx+uyYlBbjyjD7NXlbFk8+4mt3XN6F6cPaArD85YycqSvY7XKt6lsBPPO3CMa3b1rhndk47JCTG17owxPPCdwaQnhbjtpcUcqNFdWb9Q2InnVR1s2QUbXJ4eTuDaU/OZuXw7a0qPfmb2SJlpSTzwnSEs37aH37/XdEBK+6CwE8+rb301dBpb79oxvUgMBnjq3+uOuc6hvlnQle+M6MGfZq9lRckeR+oUb3Mk7Iwx5xpjVhpj1hhj7nBimyL1GrtmV69LWhKXFvbgXwu3ULq38pjrHern4wbQITmBO175nNqIuqG0dy0OO2NMEHgcOA8oAK40xhS0dLsi9Q6GXULjh+uNp/ehOhLhmf8Ux7TdTqmJ/OKCAhZv2s2zn2xoaZnicU607EYBa6y166y1VcCLwEUObFcEgMrq+tPYhq/Z1cvPTOWcgq48P3fjwfc05aJhuZzRP5PfvruSL8qrWlyreJcTYdcd2HTI35vrXhNxRH3LLtxEyw7g+tN6s2t/Na8tiu1hf2MMv7iggPKqWh6ZeexRVKTti9sNCmPMZGPMfGPM/LKysnjtVtqBWFt2AKf07syAbh34y8fF2BgfB+vfNZ1rTunJ83M36mZFO+ZE2G0B8g75u0fda4ex1j5hrS201hZmZWU5sFvxi1iv2UG0pXb9afms3L6X/65tfICAQ9129gmkhxO4d9ry465TvM2JsPsU6G+M6W2MSQSuADRppzjmQF3LLjmh6ZYdwPihuWSkJPDs3NhvOnRKTeTmsX359+odzFvf+HO20ja1OOystTXALcAMYDnwsrVW89eJY+pPY8Mxhl04IcilJ/fg3aLtlO6JrRsKwMTR+WSlJ/HwuytjPgWWtsORa3bW2unW2hOstX2ttfc6sU2RepXVEYIB0+BAAMdy1Sm9qIlYXp6/qemV6yQnBrllbD/mrf+COWt2HE+p4mF6gkI8r7K6lnAjHYob0jszldP7ZfLCvE3N6jB8xag8cjuGY3rOVtoWhZ14XkV1bcynsIe6clRPtuyuaFYrLSkU5IYz+jB3/Rcs2rir2fsU71LYiedVVkeOK+zOLsimU0pCs05lAa4YmUeHcIgnPortOVtpGxR24nmVNbUxdSg+UlIoyITh3ZlZtJ1dzXg6IjUpxMQxvXinqIT1O8qbvV/xJoWdeN6B4zyNBbj05DyqaiO8GuMTFfWuPTWfoDE8p2dm2w2FnXheRXVtzH3sjlSQ24FB3Ts0O+yy08N8a1AO/1y4OebnbMXbFHbieRVVx9+yA/j28B58vuVL1pQ2bxj2q0/pye791Uxbsu249y3eobATz4veoDj+Q/XCod0IGJrduhvTpwt9slJ5rhlPYoh3KezE8yqra0lqQcsuOz3M6f2zeG3RViLN6HNnjOHywjwWbtxNsW5UtHkKO/G8yupaUloQdgDfHp7Llt0VLGhm37nxw3IxBl5b3LxWoXiPwk4873g7FR/qmwU5JIUCvPXZ1ma9r1vHZEb37sJri7boedk2TmEnnldRXUtyYsvCLi0pxFknZTPt85Jmzzfx7eHdKd65n8Wbmp6XVrxLYSeeFonY436C4kgXDs1lx74DzF0X+zh3AN8amEMoYHinqKTFNYh7FHbiac0Zkr0pY0/MJiUxyJvN7ErSMSWB0X268G7Rdp3KtmEKO/G0+g69Lb1BAdEhnM46KZt3i0qoqY00673fGtiV9TvKWVvW9CTc4k0KO/G0imYO3NmU8wd3Y2d5FfOKmzca8TcLcgCYUbTdkTok/hR24mn1YdfSGxT1vn5iFuGEAG9/3rzrbzkdwwzu3pHZKzVZVFulsBNPq6hytmWXkhhi7InZvFPU/LuyZ/TPZOHGXeytrHakFokvhZ142oEaZ8MO4NxBOZTtPdDswTnP6J9FTcTyyTpNyNMWKezE0yqqojcSjnfUk4aMPSmbhKBhRjO7kozolUFKYpCPVulUti1S2ImnVTZzGsVYdAgncFq/TN4pKmlWV5KkUJBTenfm47WajKctUtiJp311g8LZQ/VbA3PY9EUFy7c1b9inUb27sK6snB37Djhaj7Q+hZ14Wn3LLinkXMsO4JsFXQkYmv1UxKjenQGY38yuK+I+hZ14WqXDXU/qZaYlUdirM+82M+wGd+9IOCHAvPWaeaytUdiJp1W0wjW7et8alMOKkr3NGqsuMRRgWF4G84qb93ytuE9hJ55WfzfWya4n9c4p6ArQ7LuyJ/fqxIptew/2AZS2QWEnnlZRXUtiMEAwYBzfdl7nFAZ179Ds63bD8jpRE7Es3fql4zVJ61HYiadFh2RvvcP03IE5LNq4m5IvK2N+z7C8DAAWb9T4dm2Jwk48rbIF0yjG4rzB3QB4Z2nswz5lpSfRPSNZg3m2MQo78TQnRiluTN+sNE7sms70pc08le2ZobBrYxR24mkVVa3bsgM4b3AOnxZ/Qene2E9lh+dlsGV3BWV71bm4rVDYiadV1kRaNI1iLM4f3A1radawT0N6RK/bLdms1l1bobATT6usqiW5FW9QAJzQNZ2TctJ5oxkzjw3q3oGAgc90KttmKOx8qKioiEGDBlFUVOR2KU2qaOUbFPXGD8tlwYZdbPpif0zrpySGOKFrOos3R7uftKXv1K8Udj5TXl7O+eefz7Jlyxg3bhzl5d6e6b61b1DUu3BILgBvLom9dTe8ZwaLNu5iz959beo79SuFnc9MmjSJ0tJSrLVs376dG264we2SGlVR1fIJsmOR1zmFk3t1atZk2Kf2zWRvZQ2XXDmxTX2nfqWw85GpU6cybdo0Kiujdx0rKyt58803mTp1qsuVHVtldXzCDuDiEd1ZtX0fn22O7cmI0/plsu/zmcx+b0ab+k79SmHnI1OmTDnqFGv//v1MmTLFpYqaVlFd68g0irEYPzSX5IQgL326Mab1O6cmsvejv1FVWXHY617/Tv1KYecj999/P6mpqYe9lpKSwgMPPOBSRY2z1sbtmh1AejiBcUO68cbirZQfqInpPRfeeDsmIemw17z8nfqZws5HJk2axLhx4wiHwwCEw2EuvPBCrr/+epcra9iBmgjWts6IJ8dyxcg8yqtqeSvGGxU/vPn7JPcdSWJSNPC8/p36mcLOZ6ZOnUp2djbGGLp27crTTz/tdknHVOnwBNmxOLlXJ/pnp/HMfzbEdKNiVO/O9L/kxySmdWoT36mfKex8JjU1lenTp1NQUMC0adOOOq31kvqBO1PidBoLYIzhhtN7s2zbnpimTAwFA4wv7EOX7/ySkwYM8Px36mcKOx8aOHAgS5cuZeDAgW6X0qj6wTHjGXYAE4Z3p0tqIk/PWRfT+hcNy4VOefz67zM8/536mcJOPKvChdPY+v1dM7oXs5aXsrZsX5Prj+jZiR6dknltcewdkiX+FHbiWfUtu3g8LnakiWN6kRQK8McP1za5rjGG8UNzmbO6rFkjp0h8tSjsjDEPGWNWGGOWGGNeNcZkOFWYyH6XTmMhOvvYxNG9+NfCzayPYUKei0d0J2LhDbXuPKulLbuZwCBr7RBgFaCelOKY+rCLVz+7I33/zL4khgI89t7qJtftl53O4O4deXXRljhUJsejRWFnrX3XWlvf+/IToEfLSxKJqqiOHlpunMZCdPj1a8fk89riLawp3dvk+heP6E7R1j2sLGl6XYk/J6/ZTQLednB74nNfncaGXKth8tf6kJoY4r7pK5pc98KhuQQDhtcXq3XnRU2GnTFmljFmaQM/Fx2yzp1ADfBcI9uZbIyZb4yZX1ZW5kz10q4d7HqS5E7LDqBLWhL/+41+vL+ilNmrGj9uM9OSOL1fJm98tjXmkVMkfpoMO2vt2dbaQQ38vA5gjLkOuAC42jbyL2ytfcJaW2itLczKynLsA0j7VX6gLuxcOo2td92pvcnvksI9by2jujbS6LoXDctl864KFmqaRc9p6d3Yc4GfAOOttbEN8SoSo/3VNSSGAoSC7vaQSgwFuHNcAWtK9/GXj9c3uu45A3NICgV4sxlDvEt8tPQo+gOQDsw0xiw2xvzJgZpEANh/oNaVbicNOXtANmcP6MojM1exceex/19PSwpx5glZvLO0hEhEp7Je0tK7sf2stXnW2mF1Pzc5VZhIeVUNqS7enDiUMYZ7JgwkFAjws1c/b/Sa3HmDcyjZU8kiTcbjKXqCQjxr/4FaUl28OXGkbh2T+el5JzFnzQ6en3fsAT6/MaArCUHDu0XNm3hbWpfCTjwrYAxXDOvNnv2xDaQZD1eP6skZ/TO5561lrClt+LnZDuEERuZ35sOV6nXgJQo78aSaWsuYvGwywon8d8Uuamq9cf0rEDA8fOlQkhOC3PbSIg7U1Da43tdPzGLl9r1s3V3R4HKJP4WdeNLCtV8SDgUxxlBZHWHh2tgmwYmHrh3CPHjJUJZu2cOv3mh4ntjT+0W7V81dvzOepUkjFHbiOcWl+ynZXXWwy0nEQsnuAxSXeqd30zcLuvI/X+/LC/M28UID1+9OzEknPSnE/OJdLlQnDVHYiecs27iP2iO6bdRGoq97ye3nnMgZ/TO567WlfHTE0xXBgGFwj44s3eKdFqnfKezEcwp6phEMmMNeCwZgYM80lypqWDBgePzqEfTLTuMHzy5g8RFdTfpkpbIuhuGhJD4UduI5+dkp5GQkUp93AQM5GUn0yk5xt7AGdAgn8MykUXROS2TiU3P5tPireSuy0sLsraw5qpUq7lDYiSeN6NuRpITo4RlOCDCib0eXKzq2rh3CvPz9MWSlJ3H1k3P5+ycbiEQslTW1hAJGgwJ4hMJOPCkUNJx6UifSk4OMOakToaBp+k0u6tYxmVd+cCqj+3bhrteWcvYjs/nbf4oZmpfh+rO9EuWNZ3FEGtAhJcTZQzPdLiNmnVIT+et1I3njs638c8FmenRO4a5xA9wuS+oo7EQcFAgYJgzvzoTh3d0uRY6g9rWI+ILCTkR8QWEnIr6gsBMRX1DYiYgvKOxExBcUdiLiCwo7EfEFhZ2I+ILCTkR8QWEnIr6gsBMRX1DYiYgvKOxExBcUdiLiCwo7EfEFhZ2I+ILCTkR8QWEnIr6gsBMRX1DYiYgvKOxExBcUdiLiCwo7EfEFhZ2I+ILCTkR8QWEnIr6gsBMRX1DYiYgvKOxExBcUdiLiCwo7EfEFhZ2I+ILCTkR8wZGwM8bcboyxxphMJ7YnIuK0FoedMSYPOAfY2PJyRERahxMtu0eBnwDWgW2JiLSKFoWdMeYiYIu19jOH6hERaRWhplYwxswCchpYdCfwM6KnsE0yxkwGJgP07NmzGSWKiLScsfb4zj6NMYOB94D9dS/1ALYCo6y1JY29t7Cw0M6fP/+49isicizGmAXW2sKGljXZsjsWa+3nQPYhOykGCq21O453myIirUX97ETEF467ZXcka22+U9sSEXGaWnYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV9Q2ImILyjsRMQXFHYi4gsKOxHxBYWdiPiCwk5EfEFhJyK+oLATEV8w1tr479SYMmBDHHeZCbTnybvb8+drz58N9Pmc1stam9XQAlfCLt6MMfOttYVu19Fa2vPna8+fDfT54kmnsSLiCwo7EfEFv4TdE24X0Mra8+drz58N9PnixhfX7ERE/NKyExGf813YGWNuN8ZYY0ym27U4xRjzkDFmhTFmiTHmVWNMhts1OcEYc64xZqUxZo0x5g6363GSMSbPGPOBMWaZMabIGHOr2zU5zRgTNMYsMsa85XYt4LOwM8bkAecAG92uxWEzgUHW2iHAKmCKy/W0mDEmCDwOnAcUAFcaYwrcrcpRNcDt1toCYDRwczv7fAC3AsvdLqKer8IOeBT4CdCuLlRaa9+11tbU/fkJ0MPNehwyClhjrV1nra0CXgQucrkmx1hrt1lrF9b9vpdoKHR3tyrnGGN6AOOAp9yupZ5vws4YcxGwxVr7mdu1tLJJwNtuF+GA7sCmQ/7eTDsKg0MZY/KB4cBcdytx1O+INiwibhdSL+R2AU4yxswCchpYdCfwM6KnsG1SY5/NWvt63Tp3Ej09ei6etcnxM8akAa8At1lr97hdjxOMMRcApdbaBcaYr7tdT712FXbW2rMbet0YMxjoDXxmjIHoad5CY8woa21JHEs8bsf6bPWMMdcBFwDfsO2jP9EWIO+Qv3vUvdZuGGMSiAbdc9baf7ldj4NOA8YbY84HwkAHY8yz1tpr3CzKl/3sjDHFQKG1tl08gG2MORd4BDjTWlvmdj1OMMaEiN5s+QbRkPsUuMpaW+RqYQ4x0f91nwG+sNbe5nY9raWuZfcja+0Fbtfim2t27dwfgHRgpjFmsTHmT24X1FJ1N1xuAWYQvXj/cnsJujqnAROBs+r+zRbXtYSklfiyZSci/qOWnYj4gsJORHxBYScivqCwExFfUNiJiC8o7ETEFxR2IuILCjsR8YX/D95yemferxoyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charges are [-0.89292929 -0.89537468 -0.87927981]\n"
          ]
        }
      ],
      "source": [
        "test_idx = np.random.randint(150)\n",
        "plot_example(simulation_train[test_idx], simulation_continued_train[test_idx])\n",
        "print(f'Charges are {charges_train[test_idx]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883762b1",
      "metadata": {
        "id": "883762b1"
      },
      "source": [
        "# Task 3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c1ddabe",
      "metadata": {
        "id": "4c1ddabe"
      },
      "source": [
        "## Data Handling and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd9df856",
      "metadata": {
        "id": "bd9df856"
      },
      "outputs": [],
      "source": [
        "#todo\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_padding(dataset):\n",
        "  return pad_sequence([torch.from_numpy(x) for x in dataset],batch_first=True)\n",
        "\n",
        "def data_loader(dataset, batch_size, shuffle=False):\n",
        "  return DataLoader(dataset, batch_size, shuffle=shuffle)"
      ],
      "metadata": {
        "id": "ghJe89uzHGz2"
      },
      "id": "ghJe89uzHGz2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_padding = data_padding(simulation_train)\n",
        "valid_padding = data_padding(simulation_valid)\n",
        "test_padding = data_padding(simulation_test)\n",
        "train_label = charges_train\n",
        "valid_label = charges_valid\n",
        "test_label = charges_test"
      ],
      "metadata": {
        "id": "V1QAa1oBHmQi"
      },
      "id": "V1QAa1oBHmQi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd9b7c1",
      "metadata": {
        "id": "7dd9b7c1"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = torch.tensor(data).float()\n",
        "        self.targets = torch.tensor(targets).float()\n",
        "           \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, index):  # get the label and data by index\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(train_padding, train_label)\n",
        "valid_dataset = Dataset(valid_padding, valid_label)\n",
        "test_dataset = Dataset(test_padding, test_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbmAuySDG6EQ",
        "outputId": "e9e82d95-fa82-4877-8526-ccb396d9bd7a"
      },
      "id": "MbmAuySDG6EQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8853f6",
      "metadata": {
        "id": "cc8853f6"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8794a0cf",
      "metadata": {
        "id": "8794a0cf"
      },
      "outputs": [],
      "source": [
        "#todo\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "Batchsize = 1\n",
        "input_dim=2\n",
        "hidden_dim = 256\n",
        "n_layers=2\n",
        "output_dim=3 #output thress charges\n",
        "drop_prob=0.5\n",
        "lr=0.001\n",
        "num_epochs = 50\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device=torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d62b5aa6",
      "metadata": {
        "id": "d62b5aa6"
      },
      "outputs": [],
      "source": [
        "class RNNChargePrediction(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input=input_dim, n_hidden=hidden_dim, num_layers=n_layers, n_output=output_dim, drop_prob=drop_prob, lr = lr):\n",
        "        super(RNNChargePrediction, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_input = n_input\n",
        "        self.n_output = output_dim\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=n_input, hidden_size=n_hidden, num_layers=num_layers, dropout=self.drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_output)\n",
        "        #self.fc1 = nn.Linear(n_hidden, n_output)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "       \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        # By default,PyTorch's LSTM initiates hidden state and cell state of zeros\n",
        "        # x.shape == (100, 220)      \n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        #out, _ = self.lstm(x, (h0, c0)) \n",
        "        out, _ = self.lstm(x) \n",
        "\n",
        "        # retrieve final hidden output of last timestep for each sequence\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        last_timestep = out[:,-1] \n",
        "\n",
        "        #apply dropout\n",
        "        last_timestep = self.drop(last_timestep)\n",
        "        #y_pred = self.fc1(last_timestep)\n",
        "        # feed lstm output to MLP, apply ReLU nonlinearity\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        h = self.drop(self.fc1(last_timestep))\n",
        "        h = nn.ReLU()(h)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        # shape [batch_size, num_classes]\n",
        "        y_pred = self.fc2(h)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNChargePrediction1(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input=input_dim, n_hidden=hidden_dim, num_layers=n_layers, n_output=output_dim, drop_prob=drop_prob, lr = lr):\n",
        "        super(RNNChargePrediction, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_input = n_input\n",
        "        self.n_output = output_dim\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=n_input, hidden_size=n_hidden, num_layers=num_layers, dropout=self.drop_prob)\n",
        "        #self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
        "        #self.fc2 = nn.Linear(n_hidden, n_output)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_output)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "       \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        # By default,PyTorch's LSTM initiates hidden state and cell state of zeros\n",
        "        # x.shape == (100, 220)      \n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        #out, _ = self.lstm(x, (h0, c0)) \n",
        "        out, _ = self.lstm(x) \n",
        "\n",
        "        # retrieve final hidden output of last timestep for each sequence\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        last_timestep = out[:,-1] \n",
        "\n",
        "        #apply dropout\n",
        "        last_timestep = self.drop(last_timestep)\n",
        "        #y_pred = self.fc1(last_timestep)\n",
        "        # feed lstm output to MLP, apply ReLU nonlinearity\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        h = self.fc1(last_timestep)\n",
        "        #h = nn.ReLU()(h)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        # shape [batch_size, num_classes]\n",
        "        y_pred = h\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "LUiM_n8hkrUX"
      },
      "id": "LUiM_n8hkrUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNChargePrediction2(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input=input_dim, n_hidden=hidden_dim, num_layers=1, n_output=output_dim, drop_prob=drop_prob, lr = lr):\n",
        "        super(RNNChargePrediction, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_input = n_input\n",
        "        self.n_output = output_dim\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=n_input, hidden_size=n_hidden, num_layers=num_layers, dropout=self.drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, n_output)\n",
        "        #self.fc1 = nn.Linear(n_hidden, n_output)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "       \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        # By default,PyTorch's LSTM initiates hidden state and cell state of zeros\n",
        "        # x.shape == (100, 220)      \n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        #out, _ = self.lstm(x, (h0, c0)) \n",
        "        out, _ = self.lstm(x) \n",
        "\n",
        "        # retrieve final hidden output of last timestep for each sequence\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        last_timestep = out[:,-1] \n",
        "\n",
        "        #apply dropout\n",
        "        last_timestep = self.drop(last_timestep)\n",
        "        #y_pred = self.fc1(last_timestep)\n",
        "        # feed lstm output to MLP, apply ReLU nonlinearity\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        h = self.drop(self.fc1(last_timestep))\n",
        "        h = nn.ReLU()(h)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        # shape [batch_size, num_classes]\n",
        "        y_pred = self.fc2(h)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "QAUZSKsMlGcq"
      },
      "id": "QAUZSKsMlGcq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0e443b7f",
      "metadata": {
        "id": "0e443b7f"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5b0aca",
      "metadata": {
        "id": "5a5b0aca"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58348edd",
      "metadata": {
        "id": "58348edd"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs, loss_fuc, Batchsize):\n",
        "    train_loader = data_loader(train_dataset, Batchsize, shuffle=True)\n",
        "    valid_loader = data_loader(valid_dataset, Batchsize, shuffle=True)\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuary = []\n",
        "    val_accuary = []\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        train_count = 0\n",
        "        model.train()\n",
        "        print(\"Starting epoch \" + str(epoch+1))\n",
        "        for (features, labels) in train_loader:\n",
        "            # Forward\n",
        "            (features, labels) = (features.to(device), labels.to(device))\n",
        "            pred = model(features)\n",
        "            loss = loss_fuc(pred, labels)\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            train_acc += (labels == pred).sum().item()\n",
        "            train_count += labels.size(0)\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_train_acc = train_acc/train_count\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accuary.append(avg_train_acc)\n",
        "\n",
        "        val_running_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        valid_count = 0\n",
        "        # check validation loss after every epoch\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for (features, labels) in valid_loader:\n",
        "                (features, labels) = (features.to(device), labels.to(device))\n",
        "                pred = model(features)\n",
        "                loss = loss_fuc(pred, labels)\n",
        "                val_running_loss += loss.item()\n",
        "                valid_acc += (labels == pred).sum().item()\n",
        "                valid_count += labels.size(0)\n",
        "        avg_val_loss = val_running_loss / len(valid_loader)\n",
        "        avg_val_acc = valid_acc/valid_count\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuary.append(avg_val_acc)\n",
        "\n",
        "        print('Epoch [{}/{}],Train Loss: {:.4f}, Train Accuary: {:.4f}, Valid Loss: {:.4f}, Valid Accuary: {:.4f}'\n",
        "              .format(epoch+1, num_epochs, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc))\n",
        "    print(\"Finished Training\")\n",
        "    return train_losses, val_losses, train_accuary, val_accuary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "176b3ea3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "176b3ea3",
        "outputId": "daa95149-4743-4aef-ac3a-dd62149fa1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1\n",
            "Epoch [1/50],Train Loss: 0.1693, Train Accuary: 0.0000, Valid Loss: 0.1167, Valid Accuary: 0.0000\n",
            "Starting epoch 2\n",
            "Epoch [2/50],Train Loss: 0.0909, Train Accuary: 0.0000, Valid Loss: 0.0945, Valid Accuary: 0.0000\n",
            "Starting epoch 3\n",
            "Epoch [3/50],Train Loss: 0.0835, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 4\n",
            "Epoch [4/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 5\n",
            "Epoch [5/50],Train Loss: 0.0829, Train Accuary: 0.0000, Valid Loss: 0.0920, Valid Accuary: 0.0000\n",
            "Starting epoch 6\n",
            "Epoch [6/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 7\n",
            "Epoch [7/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0920, Valid Accuary: 0.0000\n",
            "Starting epoch 8\n",
            "Epoch [8/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0915, Valid Accuary: 0.0000\n",
            "Starting epoch 9\n",
            "Epoch [9/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0913, Valid Accuary: 0.0000\n",
            "Starting epoch 10\n",
            "Epoch [10/50],Train Loss: 0.0832, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 11\n",
            "Epoch [11/50],Train Loss: 0.0831, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 12\n",
            "Epoch [12/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 13\n",
            "Epoch [13/50],Train Loss: 0.0829, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 14\n",
            "Epoch [14/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 15\n",
            "Epoch [15/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0920, Valid Accuary: 0.0000\n",
            "Starting epoch 16\n",
            "Epoch [16/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 17\n",
            "Epoch [17/50],Train Loss: 0.0830, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 18\n",
            "Epoch [18/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0914, Valid Accuary: 0.0000\n",
            "Starting epoch 19\n",
            "Epoch [19/50],Train Loss: 0.0832, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 20\n",
            "Epoch [20/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 21\n",
            "Epoch [21/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 22\n",
            "Epoch [22/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0915, Valid Accuary: 0.0000\n",
            "Starting epoch 23\n",
            "Epoch [23/50],Train Loss: 0.0831, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Starting epoch 24\n",
            "Epoch [24/50],Train Loss: 0.0832, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 25\n",
            "Epoch [25/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0921, Valid Accuary: 0.0000\n",
            "Starting epoch 26\n",
            "Epoch [26/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0915, Valid Accuary: 0.0000\n",
            "Starting epoch 27\n",
            "Epoch [27/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 28\n",
            "Epoch [28/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 29\n",
            "Epoch [29/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0923, Valid Accuary: 0.0000\n",
            "Starting epoch 30\n",
            "Epoch [30/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 31\n",
            "Epoch [31/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0915, Valid Accuary: 0.0000\n",
            "Starting epoch 32\n",
            "Epoch [32/50],Train Loss: 0.0824, Train Accuary: 0.0000, Valid Loss: 0.0916, Valid Accuary: 0.0000\n",
            "Starting epoch 33\n",
            "Epoch [33/50],Train Loss: 0.0829, Train Accuary: 0.0000, Valid Loss: 0.0920, Valid Accuary: 0.0000\n",
            "Starting epoch 34\n",
            "Epoch [34/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 35\n",
            "Epoch [35/50],Train Loss: 0.0824, Train Accuary: 0.0000, Valid Loss: 0.0920, Valid Accuary: 0.0000\n",
            "Starting epoch 36\n",
            "Epoch [36/50],Train Loss: 0.0830, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Starting epoch 37\n",
            "Epoch [37/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Starting epoch 38\n",
            "Epoch [38/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0915, Valid Accuary: 0.0000\n",
            "Starting epoch 39\n",
            "Epoch [39/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0916, Valid Accuary: 0.0000\n",
            "Starting epoch 40\n",
            "Epoch [40/50],Train Loss: 0.0832, Train Accuary: 0.0000, Valid Loss: 0.0912, Valid Accuary: 0.0000\n",
            "Starting epoch 41\n",
            "Epoch [41/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 42\n",
            "Epoch [42/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0917, Valid Accuary: 0.0000\n",
            "Starting epoch 43\n",
            "Epoch [43/50],Train Loss: 0.0825, Train Accuary: 0.0000, Valid Loss: 0.0916, Valid Accuary: 0.0000\n",
            "Starting epoch 44\n",
            "Epoch [44/50],Train Loss: 0.0823, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 45\n",
            "Epoch [45/50],Train Loss: 0.0826, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Starting epoch 46\n",
            "Epoch [46/50],Train Loss: 0.0828, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Starting epoch 47\n",
            "Epoch [47/50],Train Loss: 0.0829, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Starting epoch 48\n",
            "Epoch [48/50],Train Loss: 0.0827, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 49\n",
            "Epoch [49/50],Train Loss: 0.0830, Train Accuary: 0.0000, Valid Loss: 0.0918, Valid Accuary: 0.0000\n",
            "Starting epoch 50\n",
            "Epoch [50/50],Train Loss: 0.0830, Train Accuary: 0.0000, Valid Loss: 0.0919, Valid Accuary: 0.0000\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "model= RNNChargePrediction().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "criterion = nn.MSELoss()\n",
        "train_losses, val_losses, train_accuary, val_accuary = train(model=model,num_epochs=num_epochs,loss_fuc = criterion, Batchsize=Batchsize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8d747c",
      "metadata": {
        "id": "cf8d747c"
      },
      "outputs": [],
      "source": [
        "def plot_loss(train_loss, val_loss,epoche):\n",
        "    \"\"\"\n",
        "    plot the loss change during the training precedure\n",
        "    \"\"\"\n",
        "    plt.title(\"Train and validation loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    epoche_range=[i+1 for i in range(epoche)]\n",
        "    plt.plot(epoche_range, train_loss)\n",
        "    plt.plot(epoche_range, val_loss)\n",
        "    plt.legend(['train loss', 'validation loss'], loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a007157",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4a007157",
        "outputId": "ce13141c-7e8b-403d-819b-04efedb64755"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plot_loss(train_loss=train_losses,val_loss=val_losses,epoche=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da139d5b",
      "metadata": {
        "id": "da139d5b"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebed03ba",
      "metadata": {
        "id": "ebed03ba"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a3422e",
      "metadata": {
        "id": "43a3422e"
      },
      "outputs": [],
      "source": [
        "## margin of error ##\n",
        "## purpose: for defining the correctness of the model prediction, \n",
        "## i.e. the distance between actual particle position and predicted particle position should be less than this margin \n",
        "\n",
        "deltas = [0.2, 0.4, 0.6, 0.8, 1, 1.2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_dataset, delta, loss_func):\n",
        "  model.to(device)\n",
        "  test_loader = data_loader(test_dataset, Batchsize)\n",
        "  test_running_loss = 0.0\n",
        "  l1_test_acc = 0.0\n",
        "  l_inf_test_acc = 0.0\n",
        "  test_count = 0\n",
        "\n",
        "  with torch.no_grad():    \n",
        "    for (features, labels) in test_loader:\n",
        "    #for (feature, label) in test_dataset:\n",
        "      (features, labels) = (features.to(device), labels.to(device))\n",
        "      pred = model(features)\n",
        "      #pred = model(feature)\n",
        "      loss = loss_func(pred, labels)\n",
        "      test_running_loss += loss\n",
        "      l1_test_acc += l1_correct_count(pred, labels, delta)\n",
        "      l_inf_test_acc +=l_inf_correct_count(pred, labels, delta)\n",
        "      test_count += labels.size(0)\n",
        "    avg_val_loss = test_running_loss / len(test_loader)\n",
        "    l1_avg_val_acc = l1_test_acc/test_count\n",
        "    l_inf_avg_val_acc = l_inf_test_acc/test_count\n",
        "\n",
        "  print('Margin: {}, Test Loss: {:.4f}, L1 Test Margin Accuary: {:.4f}, L-Infinity Test Margin Accuary: {:.4f}'.format(delta, avg_val_loss, l1_avg_val_acc, l_inf_avg_val_acc))\n",
        "\n",
        "def l1_correct_count(pred, labels, delta):\n",
        "  diff = torch.sum(abs(labels-pred), dim = 1)\n",
        "  count = 0.0\n",
        "  for item in diff:\n",
        "    if item <= delta:\n",
        "      count += 1\n",
        "  \n",
        "  return count\n",
        "\n",
        "def l_inf_correct_count(pred, labels, delta):\n",
        "  diff, _ = torch.max(abs(labels-pred), dim = 1)\n",
        "  count = 0.0\n",
        "  for item in diff:\n",
        "    if item <= delta:\n",
        "      count += 1\n",
        "  \n",
        "  return count"
      ],
      "metadata": {
        "id": "0iEDsdSxFgg5"
      },
      "id": "0iEDsdSxFgg5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for delta in deltas:\n",
        "  test(model, test_dataset, delta, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNL_h6Eim11C",
        "outputId": "7e1c88cd-ea6b-472c-cd1b-31d434eaea29"
      },
      "id": "ZNL_h6Eim11C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Margin: 0.2, Test Loss: 0.0873, L1 Test Margin Accuary: 0.0300, L-Infinity Test Margin Accuary: 0.0900\n",
            "Margin: 0.4, Test Loss: 0.0873, L1 Test Margin Accuary: 0.1100, L-Infinity Test Margin Accuary: 0.5100\n",
            "Margin: 0.6, Test Loss: 0.0873, L1 Test Margin Accuary: 0.3100, L-Infinity Test Margin Accuary: 1.0000\n",
            "Margin: 0.8, Test Loss: 0.0873, L1 Test Margin Accuary: 0.5500, L-Infinity Test Margin Accuary: 1.0000\n",
            "Margin: 1, Test Loss: 0.0873, L1 Test Margin Accuary: 0.7200, L-Infinity Test Margin Accuary: 1.0000\n",
            "Margin: 1.2, Test Loss: 0.0873, L1 Test Margin Accuary: 0.9300, L-Infinity Test Margin Accuary: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a59808b",
      "metadata": {
        "id": "9a59808b"
      },
      "source": [
        "# Task 3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f64a855d",
      "metadata": {
        "id": "f64a855d"
      },
      "source": [
        "## Data Handling and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b935865",
      "metadata": {
        "id": "5b935865"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec19a8d",
      "metadata": {
        "id": "0ec19a8d"
      },
      "outputs": [],
      "source": [
        "train_feature_2 = data_padding(simulation_train_task32)\n",
        "valid_feature_2 = valid_padding\n",
        "test_feature_2 = test_padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f189d19",
      "metadata": {
        "id": "8f189d19"
      },
      "outputs": [],
      "source": [
        "def padding_masking(dataset):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    mask_last = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "      x = dataset[i][:,0]\n",
        "      x_list.append(x)\n",
        "      y = dataset[i][:,1]\n",
        "      y_list.append(y)\n",
        "      mask_last.append(len(x))\n",
        "\n",
        "    x_padding=pad_sequence([torch.from_numpy(i) for i in x_list]).transpose(0,1).float()\n",
        "    y_padding=pad_sequence([torch.from_numpy(i) for i in x_list]).transpose(0,1).float()\n",
        "    max_len = len(x_padding[0])\n",
        "\n",
        "    data_padding = torch.cat((x_padding,y_padding),1)\n",
        "    masking = torch.tensor(create_masking(mask_last,max_len)).float()\n",
        "    return torch.stack([data_padding, masking],dim=1)\n",
        "\n",
        "def create_masking (mask_last, max_len):\n",
        "  masking = []\n",
        "  for i in range(len(mask_last)):\n",
        "    x_mask = torch.ones(max_len)\n",
        "    y_mask = torch.ones(max_len)\n",
        "    x_mask[mask_last[i]:] = 0\n",
        "    y_mask[mask_last[i]:] = 0\n",
        "    masking.append(np.hstack((x_mask,y_mask)))\n",
        "  return masking"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_label_2 = padding_masking(simulation_continued_train)\n",
        "valid_label_2 = padding_masking(simulation_continued_valid)\n",
        "test_label_2 = padding_masking(simulation_continued_test)"
      ],
      "metadata": {
        "id": "z4pFHi_3gIvn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "5cd49aed-8901-49a2-f9c3-f732d2a21b24"
      },
      "id": "z4pFHi_3gIvn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-951dd6dc91c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_label_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_masking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_continued_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_label_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_masking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_continued_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_label_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_masking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimulation_continued_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-61996934ef3b>\u001b[0m in \u001b[0;36mpadding_masking\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmask_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mx_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Since test length is 118, here add two zeros\n",
        "#pad = nn.ZeroPad2d(padding=(0,2,0,0))\n",
        "#test_label_2 = pad(test_label_2)"
      ],
      "metadata": {
        "id": "O5zOFVT70gLV"
      },
      "id": "O5zOFVT70gLV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_2(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = torch.tensor(data).float()\n",
        "        self.targets = torch.tensor(targets).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "    def __getitem__(self, index):  # get the label and data by index\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "        return x, y\n",
        "        \n",
        "train_dataset_2 = Dataset_2(train_feature_2, train_label_2)\n",
        "valid_dataset_2 = Dataset_2(valid_feature_2, valid_label_2)\n",
        "test_dataset_2 = Dataset_2(test_feature_2, test_label_2)"
      ],
      "metadata": {
        "id": "u_vr-YSvge39"
      },
      "id": "u_vr-YSvge39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "867aabb3",
      "metadata": {
        "id": "867aabb3"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36fe2739",
      "metadata": {
        "id": "36fe2739"
      },
      "outputs": [],
      "source": [
        "#todo\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "Batchsize_2 = 50\n",
        "input_dim_2=2\n",
        "hidden_dim_2= 256\n",
        "n_layers_2= 2\n",
        "output_dim_2= 120 \n",
        "drop_prob_2=0.5\n",
        "lr_2=0.001\n",
        "num_epochs_2 = 200\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device=torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80b1ca2",
      "metadata": {
        "id": "f80b1ca2"
      },
      "outputs": [],
      "source": [
        "class RNNTrajectoryPrediction(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_input=input_dim_2, n_hidden=hidden_dim_2, num_layers=n_layers_2, n_output=output_dim_2, drop_prob=drop_prob_2, lr = lr_2):\n",
        "        super(RNNTrajectoryPrediction, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_input = n_input\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=n_input, hidden_size=n_hidden, num_layers=num_layers, dropout=self.drop_prob, batch_first=True)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_hidden) \n",
        "        self.fc2 = nn.Linear(n_hidden, n_output)\n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "        #self.lstm = nn.LSTM(input_size=n_input, hidden_size=n_output, num_layers=num_layers, dropout=self.drop_prob, batch_first=True)\n",
        "        #self.lstm2 = nn.LSTM(input_size=n_input, hidden_size=n_output, num_layers=num_layers, dropout=self.drop_prob, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        # By default,PyTorch's LSTM initiates hidden state and cell state of zeros\n",
        "        # x.shape == (100, 220)      \n",
        "        # Forward propagate LSTM  \n",
        "        out, _ = self.lstm(x) \n",
        "        \n",
        "        # retrieve final hidden output of last timestep for each sequence\n",
        "        # shape [batch_size, hidden_dim]\n",
        "        last_timestep = out[:,-1]\n",
        "        #apply dropout\n",
        "        last_timestep = self.drop(last_timestep)\n",
        "        \n",
        "        h = self.drop(self.fc1(last_timestep))\n",
        "        \n",
        "        h = nn.ReLU()(h)\n",
        "      \n",
        "        y_pred = self.fc2(last_timestep)\n",
        "        \n",
        "        #out = self.drop(out[:,-1])\n",
        "        # Decode the hidden state of the last time step\n",
        "        # shape [batch_size, num_classes]\n",
        "        \n",
        "        return y_pred\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826fae3f",
      "metadata": {
        "id": "826fae3f"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3fce95",
      "metadata": {
        "id": "db3fce95"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_2(model, num_epochs, loss_fuc, Batchsize):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_pred = []\n",
        "    valid_pred = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loader_2 = data_loader(train_dataset_2, Batchsize_2, shuffle=True)\n",
        "        valid_loader_2 = data_loader(valid_dataset_2, Batchsize_2, shuffle=True)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        print(\"Starting epoch \" + str(epoch+1))\n",
        "        for (features, labels) in train_loader_2:\n",
        "            # Forward\n",
        "            features = features.to(device)\n",
        "            maskings = labels[:,1].to(device)\n",
        "            length_label = []\n",
        "            for mask in maskings:\n",
        "              index = len(mask)-1\n",
        "              while mask[index] == 0:\n",
        "                index -= 1 \n",
        "              length_label.append(index+1)\n",
        "            length_label = torch.tensor(length_label)\n",
        "            #print(length_label.shape)\n",
        "            labels =labels[:,0].reshape(50,2,60).transpose(dim0=1, dim1=2).float().to(device) \n",
        "            pred = model(features) \n",
        "            mask_pred = pred*maskings\n",
        "            loss = loss_fuc(mask_pred.reshape(50,2,60).transpose(dim0=1, dim1=2).float(), labels, length_label)\n",
        "            if epoch == num_epochs-1:\n",
        "              reshape_preds = train_reshape_pred(mask_pred)\n",
        "              for pred in reshape_preds:\n",
        "                train_pred.append(pred)\n",
        "            # Backward and optimize\n",
        "            optimizer_2.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "            optimizer_2.step()\n",
        "            running_loss += loss.item()\n",
        "            #train_count += labels.size(0)\n",
        "        avg_train_loss = running_loss / len(train_loader_2)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        val_running_loss = 0.0\n",
        "        #val_count = 0\n",
        "        # check validation loss after every epoch\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for (features, labels) in valid_loader_2:\n",
        "                features = features.to(device)\n",
        "                maskings = labels[:,1].to(device)\n",
        "                length_label = []\n",
        "                for mask in maskings:\n",
        "                  index = len(mask)-1\n",
        "                  while mask[index] == 0:\n",
        "                    index -= 1 \n",
        "                  length_label.append(index+1)\n",
        "                length_label = torch.tensor(length_label)\n",
        "                labels =labels[:,0].reshape(50,2,60).transpose(dim0=1, dim1=2).float().to(device) \n",
        "                pred = model(features) \n",
        "                mask_pred = pred*maskings\n",
        "                loss = loss_fuc(mask_pred.reshape(50,2,60).transpose(dim0=1, dim1=2).float(), labels, length_label)\n",
        "                if epoch == num_epochs-1:\n",
        "                    reshape_preds = train_reshape_pred(mask_pred)\n",
        "                    for pred in reshape_preds:\n",
        "                      valid_pred.append(pred)\n",
        "                val_running_loss += loss.item()\n",
        "        avg_val_loss = val_running_loss / len(valid_loader_2)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print('Epoch [{}/{}],Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "              .format(epoch+1, num_epochs, avg_train_loss, avg_val_loss))\n",
        "    print(\"Finished Training\")\n",
        "    return train_pred, valid_pred, train_losses, val_losses\n",
        "\n",
        "def train_reshape_pred(pred): \n",
        "  x_y_pairs = []\n",
        "  for item in pred:\n",
        "    x_y_pairs.append(item.reshape(2, 60).detach().numpy())\n",
        "  \n",
        "  x_y_coor = []\n",
        "  for item in x_y_pairs:\n",
        "    ture_x_y = []\n",
        "    count = len(item[0])-1\n",
        "    while item[0][count]==0:\n",
        "      count -= 1\n",
        "    ture_x_y.append(item[0][:count+1])\n",
        "    ture_x_y.append(item[1][:count+1])\n",
        "    x_y_coor.append(np.transpose(ture_x_y))\n",
        "\n",
        "  return x_y_coor "
      ],
      "metadata": {
        "id": "G4_w9A4ck7__"
      },
      "id": "G4_w9A4ck7__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Rectified_MSELoss(pred, labels, length_label):\n",
        "  # pred and labels 50*60*2 or 50*59*2\n",
        "  diff = pred - labels\n",
        "  #print(\"diff\",diff.shape)\n",
        "  squared_diff = diff * diff\n",
        "  #print(\"squared_diff\",squared_diff.shape)\n",
        "  point_squared_diff = squared_diff.sum(dim=2) #50*60\n",
        "  #print(\"point_squared_diff\",point_squared_diff.shape)\n",
        "  each_squared_diff = point_squared_diff.sum(dim=1) # 50\n",
        "\n",
        "  average_each_squared_diff = each_squared_diff / length_label # 50\n",
        "  #print(\"each_squared_diff\",each_squared_diff,each_squared_diff.shape)\n",
        "  #print(\"average\", average_each_squared_diff,average_each_squared_diff.shape)\n",
        "  sse = average_each_squared_diff.mean()\n",
        "  #print(type(sse),sse)\n",
        "  return sse"
      ],
      "metadata": {
        "id": "A6U9SCdSxy2i"
      },
      "id": "A6U9SCdSxy2i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ddb47d",
      "metadata": {
        "id": "41ddb47d"
      },
      "outputs": [],
      "source": [
        "model_2 = RNNTrajectoryPrediction().to(device)\n",
        "optimizer_2 = torch.optim.Adam(model_2.parameters(), lr=lr_2)\n",
        "criterion_2 = Rectified_MSELoss\n",
        "train_pred, valid_pred, train_losses_2, val_losses_2 = train_2(model=model_2,num_epochs=num_epochs_2,loss_fuc = criterion_2,Batchsize=Batchsize_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee069fc",
      "metadata": {
        "id": "4ee069fc"
      },
      "outputs": [],
      "source": [
        "plot_loss(train_loss=train_losses_2,val_loss=val_losses_2,epoche=num_epochs_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = np.random.randint(150)\n",
        "plot_example(simulation_train_task32[train_idx], simulation_continued_train[train_idx], train_pred[train_idx])\n",
        "print(f'Index in Train Dataset', train_idx+1)\n",
        "valid_idx = np.random.randint(100)\n",
        "plot_example(simulation_valid[valid_idx], simulation_continued_valid[valid_idx], valid_pred[valid_idx])\n",
        "print(f'Index in Valid Dataset', valid_idx+1)"
      ],
      "metadata": {
        "id": "KivspbZxPFB_"
      },
      "id": "KivspbZxPFB_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c87278a2",
      "metadata": {
        "id": "c87278a2"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbb6137",
      "metadata": {
        "id": "2cbb6137"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf6f4b7",
      "metadata": {
        "id": "2cf6f4b7"
      },
      "outputs": [],
      "source": [
        "def test_2(model, test_dataset, loss_func, bacthsizse):\n",
        "  test_loader = data_loader(test_dataset, bacthsizse)\n",
        "  test_loss = 0.0\n",
        "  test_masking_loss = 0.0\n",
        "  mask_results = []\n",
        "  #val_count = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (features, labels) in test_loader:\n",
        "      features = features.to(device)\n",
        "      labels = labels.to(device)\n",
        "      \n",
        "      maskings = labels[:,1].to(device)\n",
        "      length_label = []\n",
        "      for mask in maskings:\n",
        "        index = len(mask)-1\n",
        "        while mask[index] == 0:\n",
        "          index -= 1 \n",
        "        length_label.append(index+1)\n",
        "      length_label = torch.tensor(length_label)\n",
        "      \n",
        "      labels = labels[:,0].reshape(50,2,59).transpose(dim0=1, dim1=2).float().to(device)\n",
        "      pred = model(features)\n",
        "      pred, _ = torch.split(pred, 118, dim=1)\n",
        "      loss = loss_func(pred.reshape(50,2,59).transpose(dim0=1, dim1=2).float(), labels, length_label)\n",
        "      test_loss += loss.item()\n",
        "      mask_pred = pred * maskings\n",
        "      reshape_preds = reshape_pred(mask_pred)\n",
        "      for pred in reshape_preds:\n",
        "        mask_results.append(pred)\n",
        "      mask_loss = loss_func(mask_pred.reshape(50,2,59).transpose(dim0=1, dim1=2).float(), labels, length_label)\n",
        "      test_masking_loss += mask_loss.item()\n",
        "   \n",
        "  avg_test_loss = test_loss / len(test_loader)\n",
        "  avg_test_masking_loss = test_masking_loss / len(test_loader)\n",
        "  print('Test Loss Without Masking: {:.4f}, Test Loss With Masking: {:.4f}'\n",
        "              .format(avg_test_loss, avg_test_masking_loss))\n",
        "  return mask_results\n",
        "\n",
        "def reshape_pred(pred): \n",
        "  #pred, _ = torch.split(pred, 118, dim=1)\n",
        "  x_y_pairs = []\n",
        "  for item in pred:\n",
        "    x_y_pairs.append(item.reshape(2,59).numpy())\n",
        "\n",
        "  x_y_coor = []\n",
        "  for item in x_y_pairs:\n",
        "    ture_x_y = []\n",
        "    count = len(item[0])-1\n",
        "    while item[0][count]==0:\n",
        "      count -= 1\n",
        "    ture_x_y.append(item[0][:count+1])\n",
        "\n",
        "    ture_x_y.append(item[1][:count+1])\n",
        "    x_y_coor.append(np.transpose(ture_x_y))\n",
        "\n",
        "  return x_y_coor "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "736c10d8",
      "metadata": {
        "id": "736c10d8"
      },
      "outputs": [],
      "source": [
        "predictions = test_2(model_2, test_dataset_2, criterion_2, Batchsize_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_idx = np.random.randint(100)\n",
        "plot_example(simulation_test[test_idx], simulation_continued_test[test_idx], predictions[test_idx])\n",
        "print(f'Index in Test Dataset', test_idx+1)"
      ],
      "metadata": {
        "id": "jMe_xTsQ8xl1"
      },
      "id": "jMe_xTsQ8xl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = []\n",
        "last_points = []\n",
        "for item in simulation_test:\n",
        "  last_one_point = item[-1]\n",
        "  last_two_point = item[-2]\n",
        "  last_points.append(last_one_point)\n",
        "  v_x = last_one_point[0]-last_two_point[0]\n",
        "  v_y = last_one_point[1]-last_two_point[1]\n",
        "  v.append([v_x, v_y])\n"
      ],
      "metadata": {
        "id": "IlAFlyiHx18q"
      },
      "id": "IlAFlyiHx18q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_length_label = []\n",
        "for item in simulation_continued_test:\n",
        "    all_length_label.append(np.size(item,0))"
      ],
      "metadata": {
        "id": "5A5zZoJKzpT6"
      },
      "id": "5A5zZoJKzpT6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G3mJKZPU2sjB"
      },
      "id": "G3mJKZPU2sjB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "for i in range(num_test):\n",
        "  num_step = all_length_label[i]\n",
        "  last_point = last_points[i]\n",
        "  last_v = v[i]\n",
        "  one_pred = []\n",
        "  count = 1\n",
        "  while count <= num_step:\n",
        "    x_pred = last_point[0] + count * last_v[0]\n",
        "    y_pred = last_point[0] + count * last_v[1]\n",
        "    one_pred.append([x_pred, y_pred])\n",
        "    count += 1\n",
        "  pred.append(one_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "20qGTUBS0pg4"
      },
      "id": "20qGTUBS0pg4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybNhEBde-g57",
        "outputId": "1e262154-379b-48da-e8d9-6198d6dd66ef"
      },
      "id": "ybNhEBde-g57",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.4629561914375695, -2.383746754453175]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = 0\n",
        "for i in range(num_test):\n",
        "  one_pred = pred[i]\n",
        "  one_true = simulation_continued_test[i]\n",
        "  one_loss = 0\n",
        "  for i in range(np.size(one_pred, 0)):\n",
        "    \n",
        "    x_loss = pow((one_pred[i][0]-one_true[i][0]),2)\n",
        "    y_loss = pow((one_pred[i][1]-one_true[i][1]),2)\n",
        "    x_y_loss = x_loss+y_loss\n",
        "    one_loss += x_y_loss\n",
        "  loss += (one_loss/(np.size(one_pred, 0)))\n",
        "loss = loss/num_test\n"
      ],
      "metadata": {
        "id": "A7qSC40p42ZP"
      },
      "id": "A7qSC40p42ZP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "UGEEdUiI43mO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f93e55c-e388-4ffe-e7a9-7b1558b096f9"
      },
      "id": "UGEEdUiI43mO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.69299040196317\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "270ffb0c874d7e512b44b8bd305fde61df9cabfb71a4d33bb975d4a2141f772c"
      }
    },
    "colab": {
      "name": "a3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}